# Лабораторная работа №9 (Часть 1): Разработка сквозного пайплайна обработки данных

**Тема:** Разработка сквозного пайплайна обработки данных: ingest данных в Kafka → потоковая обработка в Spark → запись результатов в NoSQL БД → визуализация.

**Цель работы:** Разработать комплексное решение для обработки потоковых данных в реальном времени с использованием всего изученного стека технологий. Получить практический опыт построения end-to-end data pipeline.

**Стек технологий:**
- **Платформы:** Apache Kafka, Apache Spark, MongoDB
- **Языки программирования:** Python (PySpark), JavaScript
- **Библиотеки:** kafka-python, pyspark, pymongo, dash/plotly
- **ОС:** Linux (Ubuntu) или Windows с WSL2
- **Инструменты:** Jupyter Notebook, MongoDB Compass

---

### Теоретическая часть

**Архитектура пайплайна:**
1. **Data Ingestion:** Apache Kafka для приема потоковых данных
2. **Stream Processing:** Apache Spark Structured Streaming для обработки в реальном времени
3. **Data Storage:** MongoDB для хранения результатов обработки
4. **Data Visualization:** Dash/Plotly для визуализации результатов

**Ключевые требования:**
- Обработка 1000+ сообщений в секунду
- Гарантия доставки сообщений (at-least-once semantics)
- Масштабируемость всех компонентов
- Реальное время визуализации

---

### Задание на практическую реализацию

#### Этап 1: Подготовка инфраструктуры (2 часа)

1. **Развертывание Kafka-кластера:**
```bash
# Запуск Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Запуск Kafka Broker
bin/kafka-server-start.sh config/server.properties

# Создание топика
kafka-topics.sh --create \
    --topic sensor-data \
    --bootstrap-server localhost:9092 \
    --partitions 3 \
    --replication-factor 1
```

2. **Настройка MongoDB:**
```bash
# Запуск MongoDB
mongod --dbpath /data/db

# Создание коллекций
mongo --eval "
db.createCollection('sensor_metrics');
db.createCollection('anomaly_alerts');
db.sensor_metrics.createIndex({timestamp: -1});
db.anomaly_alerts.createIndex({timestamp: -1});
"
```

#### Этап 2: Генератор данных для Kafka (3 часа)

**Реализуйте продюсер, генерирующий данные IoT-сенсоров:**
```python
from kafka import KafkaProducer
import json
import time
import random
from datetime import datetime

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    acks='all',
    retries=3
)

sensor_types = ['temperature', 'humidity', 'pressure', 'vibration']
locations = ['factory-floor', 'warehouse', 'office', 'outdoor']

def generate_sensor_data():
    while True:
        data = {
            'sensor_id': f"sensor_{random.randint(1, 1000):04d}",
            'sensor_type': random.choice(sensor_types),
            'value': round(random.uniform(0, 100), 2),
            'location': random.choice(locations),
            'timestamp': datetime.utcnow().isoformat(),
            'status': 'normal' if random.random() > 0.05 else 'anomaly'
        }
        producer.send('sensor-data', value=data)
        time.sleep(0.001)  # 1000 сообщений в секунду

generate_sensor_data()
```

#### Этап 3: Spark Streaming Processing (4 часа)

**Реализуйте обработку данных в реальном времени:**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder \
    .appName("IoTStreamProcessing") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .getOrCreate()

# Схема данных
schema = StructType([
    StructField("sensor_id", StringType()),
    StructField("sensor_type", StringType()),
    StructField("value", DoubleType()),
    StructField("location", StringType()),
    StructField("timestamp", TimestampType()),
    StructField("status", StringType())
])

# Чтение из Kafka
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "sensor-data") \
    .option("startingOffsets", "latest") \
    .load()

# Парсинг и обработка
parsed_df = kafka_df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Агрегации
windowed_metrics = parsed_df \
    .withWatermark("timestamp", "5 minutes") \
    .groupBy(
        window(col("timestamp"), "1 minute", "30 seconds"),
        col("sensor_type"),
        col("location")
    ) \
    .agg(
        avg("value").alias("avg_value"),
        max("value").alias("max_value"),
        min("value").alias("min_value"),
        count("*").alias("message_count")
    )

# Обнаружение аномалий
anomalies = parsed_df.filter(col("status") == "anomaly") \
    .withColumn("alert_type", 
        when(col("value") > 90, "CRITICAL")
        .when(col("value") > 80, "WARNING")
        .otherwise("INFO")
    )
```

#### Этап 4: Запись в MongoDB (3 часа)

**Реализуйте запись результатов в MongoDB:**
```python
def write_to_mongodb(batch_df, batch_id):
    batch_df.write \
        .format("mongo") \
        .mode("append") \
        .option("uri", "mongodb://localhost:27017") \
        .option("database", "iot_analytics") \
        .option("collection", "sensor_metrics") \
        .save()

# Запись метрик
query_metrics = windowed_metrics.writeStream \
    .foreachBatch(write_to_mongodb) \
    .outputMode("update") \
    .option("checkpointLocation", "checkpoints/metrics") \
    .start()

# Запись аномалий
query_anomalies = anomalies.writeStream \
    .foreachBatch(lambda df, id: df.write \
        .format("mongo") \
        .mode("append") \
        .option("uri", "mongodb://localhost:27017") \
        .option("database", "iot_analytics") \
        .option("collection", "anomaly_alerts") \
        .save()
    ) \
    .outputMode("append") \
    .option("checkpointLocation", "checkpoints/anomalies") \
    .start()
```

#### Этап 5: Визуализация данных (4 часа)

**Создайте Dash-приложение для визуализации:**
```python
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.express as px
from pymongo import MongoClient
import pandas as pd

client = MongoClient('mongodb://localhost:27017')
db = client.iot_analytics

app = dash.Dash(__name__)

app.layout = html.Div([
    html.H1("Real-time IoT Sensor Monitoring"),
    
    dcc.Graph(id='live-metrics'),
    dcc.Graph(id='anomaly-chart'),
    
    dcc.Interval(
        id='interval-component',
        interval=5000,  # 5 seconds
        n_intervals=0
    )
])

@app.callback(
    [Output('live-metrics', 'figure'),
     Output('anomaly-chart', 'figure')],
    [Input('interval-component', 'n_intervals')]
)
def update_graphs(n):
    # Получение данных из MongoDB
    metrics_data = list(db.sensor_metrics.find().sort('_id', -1).limit(100))
    anomalies_data = list(db.anomaly_alerts.find().sort('timestamp', -1).limit(50))
    
    # Создание графиков
    metrics_fig = px.line(metrics_data, x='window.start', y='avg_value', 
                         color='sensor_type', title='Real-time Sensor Metrics')
    
    anomaly_fig = px.bar(anomalies_data, x='timestamp', y='value',
                        color='alert_type', title='Anomaly Detection Alerts')
    
    return metrics_fig, anomaly_fig

if __name__ == '__main__':
    app.run_server(debug=True, port=8050)
```

#### Этап 6: Тестирование и мониторинг (2 часа)

**Реализуйте мониторинг пайплайна:**
```python
# Мониторинг производительности
spark.streams.addListener(lambda query: print(f"Query {query.name} status: {query.status}"))

# Метрики производительности
throughput = spark.streams.active[0].recentProgress[-1]['numInputRows']
latency = spark.streams.active[0].recentProgress[-1]['batchDuration']
```

---

### Требования к отчёту

1. **Архитектурная схема** пайплайна с описанием компонентов
2. **Графики производительности:** throughput, latency, resource usage
3. **Анализ результатов:** обработка 1000+ сообщений/сек
4. **Примеры данных** на каждом этапе пайплайна
5. **Скриншоты** визуализации и мониторинга

---

### Критерии оценки

**Отлично (5):**
- Обработка 1000+ сообщений/сек с задержкой < 1 секунды
- Полная реализация всех этапов пайплайна
- Качественная визуализация и мониторинг
- Глубокий анализ производительности

**Хорошо (4):**
- Обработка 500+ сообщений/сек
- Реализация основных этапов пайплайна
- Базовая визуализация данных

**Удовлетворительно (3):**
- Работающий пайплайн с базовой функциональностью
- Обработка 100+ сообщений/сек

---

### Рекомендуемая литература

1. **Kafka: The Definitive Guide** by Neha Narkhede, Gwen Shapira, Todd Palino
2. **Spark: The Definitive Guide** by Bill Chambers, Matei Zaharia
3. **MongoDB: The Definitive Guide** by Kristina Chodorow
4. **Designing Data-Intensive Applications** by Martin Kleppmann
5. **Stream Processing with Apache Spark** by Gerard Maas, Francois Garillot
6. Клеппман М. - "Высоконагруженные приложения. Программирование, масштабирование, поддержка" (2018)
7. Шапира Г., Палино Т., Наркхеде Н. - "Kafka: потоковая обработка и анализ данных" (2019)
8. Гарриссон Л. - "MongoDB в действии" (2020)
9. Захария М., Уэнделл П. - "Изучаем Spark: молниеносный анализ данных" (2017)
10. Сафонов В.О. - "Обработка больших данных с использованием Apache Spark" (2021)