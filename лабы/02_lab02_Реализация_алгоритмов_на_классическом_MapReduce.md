# Лабораторная работа №2: Реализация алгоритмов на классическом MapReduce

**Цель работы:** Освоить модель программирования MapReduce на практике. Получить навыки реализации алгоритмов обработки данных с использованием парадигмы MapReduce на языке Python для запуска в Hadoop Streaming.

**Стек технологий:**
*   **Язык программирования:** Python 3
*   **Фреймворк:** Hadoop Streaming (JAR-утилита)
*   **Платформа/ОС:** Linux (Ubuntu), предустановленные HDFS и Hadoop
*   **Библиотеки (опционально, для локальной отладки):** `mrjob`

### Теоретическая часть (краткое содержание из лекции):

**MapReduce** — это модель и фреймворк для параллельной обработки больших наборов данных на кластере. Вычисления разделяются на две основные фазы:

1.  **Map (Отображение):**
    *   **Вход:** Пары ключ-значение (например, `(offset, строка_из_файла)`).
    *   **Функция:** Пользовательская функция `map()` обрабатывает каждую входную пару и генерирует набор *промежуточных* пар ключ-значение.
    *   **Выход:** Список промежуточных пар вида `(промежуточный_ключ, промежуточное_значение)`.

2.  **Reduce (Свёртка):**
    *   **Вход:** Пары `(промежуточный_ключ, список_всех_промежуточных_значений)`.
    *   **Функция:** Пользовательская функция `reduce()` принимает ключ и все associated с ним значения. Она агрегирует, фильтрует или суммирует эти значения.
    *   **Выход:** Конечные пары ключ-значение, которые записываются в результат.

Между этими фазами framework неявно выполняет этап **Shuffle & Sort**, который группирует все промежуточные значения по их ключам.

### Задание на практическую реализацию:

#### Задача 1: Классический WordCount

Реализуйте алгоритм подсчёта слов (WordCount) — "Hello World" экosystemы MapReduce.

**Алгоритм:**
*   **Mapper (`mapper_wc.py`):** Читает текст из стандартного ввода, разбивает каждую строку на слова и для каждого слова выдаёт пару `(word, 1)`.
*   **Reducer (`reducer_wc.py`):** Получает на вход пары `(word, [1, 1, 1, ...])`, суммирует единицы и выдаёт итоговую пару `(word, sum)`.

**Пример:**
*Вход:*
```
hello world
hello data
goodbye data
```
*Выход Mapper-а:*
```
hello	1
world	1
hello	1
data	1
goodbye	1
data	1
```
*Выход Reducer-а:*
```
data	2
goodbye	1
hello	2
world	1
```

#### Задача 2: Вычисление средней оценки студента по предметам

Реализуйте алгоритм вычисления средней оценки для каждого студента по всем предметам.

**Входные данные:** Файл `grades.txt` в HDFS, где каждая строка имеет формат:
`student_id,subject,grade`
Пример:
```
101,math,92
101,physics,85
102,math,90
102,physics,95
103,math,80
...
```

**Алгоритм:**
*   **Mapper (`mapper_avg.py`):** Для каждой строки генерирует пару, где *ключ* — это `student_id`, а *значение* — это `grade`.
    *   *Выход:* `(101, 92)`, `(101, 85)`, `(102, 90)`, `(102, 95)`, `(103, 80)`...
*   **Reducer (`reducer_avg.py`):** Для каждого `student_id` получает список всех его оценок. Вычисляет среднее арифметическое этих оценок и выдаёт результат.
    *   *Выход:* `(101, 88.5)`, `(102, 92.5)`, `(103, 80.0)`...

### Требования к реализации:

1.  **Локальная отладка:** Протестируйте ваши скрипты локально с помощью симуляции MapReduce, прежде чем запускать в Hadoop.
    ```bash
    # Для WordCount
    cat input.txt | python3 mapper_wc.py | sort | python3 reducer_wc.py

    # Для средней оценки
    cat grades.txt | python3 mapper_avg.py | sort | python3 reducer_avg.py
    ```

2.  **Запуск в Hadoop Streaming:**
    *   Поместите входные файлы в HDFS.
    *   Используйте утилиту `hadoop streaming` для запуска джоба.
    ```bash
    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -files mapper.py,reducer.py \
    -input /user/student/input_data \
    -output /user/student/output_result \
    -mapper "python3 mapper.py" \
    -reducer "python3 reducer.py"
    ```

### Требования к оформлению и отчёту:

1.  **Код:** Предоставьте файлы `mapper_wc.py`, `reducer_wc.py`, `mapper_avg.py`, `reducer_avg.py`.
2.  **Тестовые данные:** Создайте и приложите небольшие тестовые файлы `input.txt` и `grades.txt`.
3.  **Скриншоты:**
    *   Результат локального тестирования для обеих задач.
    *   Команда запуска Hadoop Streaming job.
    *   Результат выполнения джоба в консоли (успешное завершение).
    *   Содержимое выходной директории в HDFS, показывающее успешное создание файлов `part-00000`.
    *   Вывод результатов из HDFS для обеих задач (`hdfs dfs -cat /user/student/output_result/part-00000`).
4.  **Отчёт:** Краткий письменный отчёт должен содержать:
    *   Объяснение алгоритма для каждой из задач своими словами.
    *   Ответы на вопросы:
        *   Почему этап Shuffle & Sort является необходимым и что происходит на этом этапе?
        *   В чём заключаются ограничения классической модели MapReduce, которые привели к появлению Spark?

### Критерии оценки:

*   **Удовлетворительно (3):** Реализована и успешно протестирована локально Задача 1 (WordCount).
*   **Хорошо (4):** Реализованы и успешно протестированы локально обе задачи. Задача 1 (WordCount) успешно запущена через Hadoop Streaming.
*   **Отлично (5):** Реализованы и успешно запущены через Hadoop Streaming обе задачи. В отчёте даны развёрнутые и точные ответы на теоретические вопросы, демонстрирующие понимание работы Shuffle & Sort и ограничений MapReduce.