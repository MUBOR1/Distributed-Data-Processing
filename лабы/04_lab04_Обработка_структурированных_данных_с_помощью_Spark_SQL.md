# Лабораторная работа №4: Обработка структурированных данных с помощью Spark SQL

**Цель работы:** Освоить работу с высокоуровневыми API Apache Spark — Spark SQL и DataFrames. Получить практические навыки загрузки структурированных данных, выполнения запросов с помощью SQL-синтаксиса и DataFrame API, а также проведения базового анализа и визуализации данных.

**Стек технологий:**
*   **Платформа/ОС:** Linux (Ubuntu) или Windows с WSL2
*   **Фреймворк:** Apache Spark (версии 3.x)
*   **Язык программирования:** Python (PySpark)
*   **Инструментарий:** Jupyter Lab / Jupyter Notebook
*   **Данные:** Структурированные данные в форматах JSON/CSV

### Теоретическая часть (краткое содержание):

**Spark SQL** — это модуль в Spark для работы со структурированными данными. Он предоставляет два основных API:

1.  **DataFrames:** Распределенная коллекция данных, организованная в именованные столбцы. Концептуально эквивалентна таблице в реляционной БД или dataframe в R/Pandas, но с оптимизациями под капотом (Catalyst Optimizer, Tungsten Execution Engine).
2.  **Datasets:** Типизированная версия DataFrame (доступна в Java/Scala, в Python эквивалентна DataFrame).

**Преимущества DataFrame над RDD:**
*   Высокая производительность за счет оптимизации запросов
*   Возможность использования SQL-синтаксиса
*   Встроенная поддержка схемы данных
*   Интеграция с различными источниками данных

### Задание на практическую реализацию:

#### Часть 1: Создание сессии Spark и загрузка данных

1.  **Создайте сессию Spark с поддержкой SQL:**
    ```python
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName("Lab4").getOrCreate()
    ```

2.  **Загрузите данные из CSV и JSON файлов:**
    *   Загрузите файл `employees.csv` с данными о сотрудниках (колонки: id, name, department, salary)
    *   Загрузите файл `departments.json` с данными о департаментах (колонки: dep_id, dep_name, location)
    *   Просмотрите схемы загруженных данных с помощью `.printSchema()`

#### Часть 2: Работа с DataFrame API

1.  **Базовые операции:**
    *   Покажите первые 5 записей из каждого DataFrame
    *   Отфильтруйте сотрудников с зарплатой выше 50000
    *   Сгруппируйте данные по департаментам и посчитайте среднюю зарплату
    *   Отсортируйте результат по убыванию средней зарплаты

2.  **JOIN-операции:**
    *   Объедините данные о сотрудниках и департаментах по идентификатору департамента
    *   Посчитайте количество сотрудников в каждом департаменте
    *   Найдите департамент с наибольшим количеством сотрудников

#### Часть 3: Работа с Spark SQL

1.  **Создание временных представлений:**
    ```python
    employees_df.createOrReplaceTempView("employees")
    departments_df.createOrReplaceTempView("departments")
    ```

2.  **Выполнение SQL-запросов:**
    *   Напишите SQL-запрос для поиска 3 самых высокооплачиваемых сотрудников
    *   Напишите SQL-запрос для расчета фонда оплаты труда по каждому департаменту
    *   Напишите SQL-запрос для поиска сотрудников, чья зарплата выше средней по их департаменту

#### Часть 4: Визуализация результатов

1.  **Подготовка данных для визуализации:**
    *   Создайте DataFrame с статистикой по департаментам (количество сотрудников, средняя зарплата, общий фонд оплаты труда)

2.  **Визуализация:**
    *   Используя matplotlib или seaborn, постройте:
        *   Столбчатую диаграмму средней зарплаты по департаментам
        *   Круговую диаграмму распределения сотрудников по департаментам
        *   График зависимости средней зарплаты от количества сотрудников в департаменте

### Требования к оформлению и отчёту:

1.  **Код:** Предоставьте полный код PySpark в виде Jupyter Notebook (`.ipynb`)
2.  **Данные:** Создайте и приложите тестовые CSV и JSON файлы с данными
3.  **Скриншоты:**
    *   Результаты выполнения основных операций (первые записи, результаты фильтрации, группировки)
    *   Результаты SQL-запросов
    *   Графики визуализации
4.  **Отчёт:** Включите:
    *   Описание преимуществ Spark SQL перед работой с RDD
    *   Сравнение производительности одинаковых операций, выполненных через DataFrame API и SQL
    *   Объяснение работы оптимизатора Catalyst

### Критерии оценки:

*   **Удовлетворительно (3):** Выполнены Части 1-2, данные загружены, основные операции выполнены
*   **Хорошо (4):** Выполнены Части 1-3, реализованы JOIN-операции и SQL-запросы
*   **Отлично (5):** Полностью выполнены все части, включая визуализацию, предоставлен полный анализ и сравнение подходов

---

**Примечание:** Для тестовых данных можно использовать следующие примеры:

**employees.csv:**
```csv
id,name,department,salary
1,Ivanov,IT,75000
2,Petrov,HR,50000
3,Sidorov,IT,80000
4,Smirnov,Sales,60000
5,Kuznetsov,HR,55000
```

**departments.json:**
```json
{"dep_id": "IT", "dep_name": "Information Technology", "location": "Floor 3"}
{"dep_id": "HR", "dep_name": "Human Resources", "location": "Floor 2"}
{"dep_id": "Sales", "dep_name": "Sales Department", "location": "Floor 1"}
```