Отлично. На основании тематического плана (файл `00_plan.pdf`), вот задание на лабораторную работу №3, соответствующее теме "Работа с RDD в Apache Spark".

---

# Лабораторная работа №3: Работа с RDD в Apache Spark

**Цель работы:** Освоить базовые принципы работы с низкоуровневой абстракцией Apache Spark — RDD (Resilient Distributed Dataset). Получить практические навыки создания RDD, применения основных трансформаций (transformations) и действий (actions), а также построения и выполнения последовательностей операций (DAG).

**Стек технологий:**
*   **Платформа/ОС:** Linux (Ubuntu) или Windows с WSL2
*   **Фреймворк:** Apache Spark (версии 3.x)
*   **Язык программирования:** Python (PySpark)
*   **Инструментарий:** Jupyter Lab / Jupyter Notebook, Spark Standalone или Local Mode
*   **Данные:** Логи веб-сервера (или иные текстовые данные)

### Теоретическая часть (краткое содержание):

**RDD (Resilient Distributed Dataset)** — это неизменяемая, отказоустойчивая коллекция объектов, распределённых across узлами кластера, которыми можно манипулировать параллельно.

*   **Resilient (Устойчивый):** Способность восстанавливаться после сбоев благодаря lineage (цепочке происхождения). RDD запоминает последовательность операций, применённых к нему, чтобы пересчитать потерянные части данных.
*   **Distributed (Распределённый):** Данные разделены на партиции (partitions) и обрабатываются на разных узлах кластера.
*   **Dataset (Набор данных):** Представляет собой набор данных (объектов).

**Операции над RDD:**
*   **Трансформации (Transformations):** "Ленивые" операции, которые создают новое RDD из существующего (например, `map`, `filter`, `flatMap`). Они не выполняются немедленно, а лишь строят план вычислений (DAG).
*   **Действия (Actions):** Операции, которые запускают вычисления для возврата результата в драйвер-программу или сохранения его во внешнее хранилище (например, `count`, `collect`, `take`, `saveAsTextFile`). Они *заставляют* выполниться все трансформации в lineage.

### Задание на практическую реализацию:

#### Часть 1: Базовые операции с RDD

1.  **Создание RDD:**
    *   Создайте RDD из локальной коллекции (списка) чисел от 1 до 1000 с помощью `sc.parallelize()`.
    *   Создайте RDD из текстового файла `log.txt` с логами веб-сервера с помощью `sc.textFile()`.

2.  **Применение трансформаций:**
    *   Для RDD с числами:
        *   Примените трансформацию `filter()`, чтобы оставить только чётные числа.
        *   Примените трансформацию `map()`, чтобы возвести каждое число в квадрат.
    *   Для RDD с логами:
        *   Примените `map()`, чтобы преобразовать каждую строку в верхний регистр.
        *   Примените `filter()`, чтобы оставить только строки, содержащие слово "ERROR".

3.  **Применение действий:**
    *   Для RDD с числами: используйте `count()`, чтобы подсчитать количество элементов, и `take(10)`, чтобы взять первые 10 элементов для просмотра.
    *   Для RDD с логами: используйте `count()`, чтобы подсчитать количество строк с ошибками.

#### Часть 2: Анализ логов веб-сервера

Используйте предоставленный файл с логами веб-сервера (`access_log` или `server_log.txt` в формате Common Log Format).

**Задача:** Найти 10 самых часто запрашиваемых URL (эндпоинтов) на сервере.

**Алгоритм решения:**
1.  **Загрузка данных:** Создайте RDD из файла логов.
2.  **Преобразование:** Используйте `map()`, чтобы преобразовать каждую строку лога в URL. Вам потребуется написать функцию, которая разбивает строку лога и извлекает URL (обычно это 7-й элемент при split по пробелу, но может потребоваться обработка исключений).
3.  **Сопоставление:** Используйте `map()`, чтобы создать пары `(url, 1)`.
4.  **Агрегация:** Используйте `reduceByKey()`, чтобы просуммировать количество вхождений для каждого URL.
5.  **Сортировка:** Используйте `sortBy()` (или преобразуйте в пары и используйте `sortByKey` с параметром `ascending=False`), чтобы отсортировать результаты по убыванию количества обращений.
6.  **Результат:** Используйте действие `take(10)`, чтобы получить топ-10 URL.

**Пример шага 2 (функция для извлечения URL):**
```python
def extract_url(line):
    try:
        # Пример для Common Log Format: "127.0.0.1 - - [01/Jan/2023:00:00:01] "GET /index.html HTTP/1.0" 200 1234"
        parts = line.split()
        if len(parts) > 6:
            return parts[6]  # Возвращает '/index.html'
        else:
            return '' # или обработать иначе
    except:
        return '' # Обработка ошибок парсинга

url_rdd = log_rdd.map(extract_url)
```

### Требования к оформлению и отчёту:

1.  **Код:** Предоставьте полный код PySpark для обеих частей задания в виде Jupyter Notebook (`.ipynb`) или Python-скрипта (`.py`).
2.  **Данные:** Приложите или укажите ссылку на небольшой тестовый файл с логами, на котором проводилась работа.
3.  **Скриншоты:**
    *   Скриншот ячейки Jupyter Notebook с кодом и результатом выполнения для Части 1 (покажите результаты `take(10)` после фильтрации и маппинга).
    *   Скриншот ячейки/вывода с результатом выполнения Части 2 (топ-10 URL).
    *   Скриншот интерфейса Spark Web UI (порт 4040) на вкладке «Stages», показывающий DAG визуализацию или детали выполнения вашего job для анализа логов.
4.  **Отчёт:** Краткий письменный отчёт должен содержать:
    *   Объяснение, чем трансформация (например, `map`) отличается от действия (например, `count`) с точки зрения выполнения и результата.
    *   Объяснение своими словами, как операция `reduceByKey` работает и почему она более эффективна, чем `groupByKey` для подобных задач агрегации.
    *   Ответ на вопрос: почему RDD считаются «низкоуровневой» абстракцией по сравнению с DataFrame в Spark?

### Критерии оценки:

*   **Удовлетворительно (3):** Выполнена Часть 1. Продемонстрировано создание RDD и применение хотя бы по одной трансформации и действию для каждого набора данных.
*   **Хорошо (4):** Полностью выполнена Часть 1 и Часть 2. Задача по анализу логов решена корректно, результат выведен. Предоставлены требуемые скриншоты.
*   **Отлично (5):** Полностью выполнены все части задания. В отчёте даны развёрнутые и точные ответы на все теоретические вопросы, демонстрирующие понимание работы трансформаций, действий, особенностей `reduceByKey` и различий между RDD и DataFrame.

---