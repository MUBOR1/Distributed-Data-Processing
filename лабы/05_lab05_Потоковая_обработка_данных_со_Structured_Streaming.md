# Лабораторная работа №5: Потоковая обработка данных со Structured Streaming

**Цель работы:** Освоить основы потоковой обработки данных с помощью Apache Spark Structured Streaming. Получить практические навыки чтения потоковых данных, выполнения скользящих агрегаций и визуализации результатов в реальном времени.

**Стек технологий:**
*   **Платформа/ОС:** Linux (Ubuntu) или Windows с WSL2
*   **Фреймворк:** Apache Spark (версии 3.x)
*   **Язык программирования:** Python (PySpark)
*   **Инструментарий:** Jupyter Lab / Jupyter Notebook, Netcat (для генерации потока данных)
*   **Данные:** Потоковые данные через socket или файловый источник

### Теоретическая часть (краткое содержание):

**Structured Streaming** — это масштабируемый и отказоустойчивый механизм потоковой обработки данных, построенный на движке Spark SQL. 

**Ключевые концепции:**
*   **Непрерывные инкрементальные вычисления:** Обработка данных происходит по мере их поступления
*   **Event-time обработка:** Возможность обработки данных по времени события, а не времени получения
*   **Водяные знаки (Watermarks):** Механизм обработки задержанных данных
*   **Output modes:** Complete, Update, Append - различные режимы вывода результатов

**Основные источники данных:**
*   Socket source (для тестирования)
*   File source
*   Kafka source

### Задание на практическую реализацию:

#### Часть 1: Настройка потока данных через Socket

1.  **Запустите netcat-сервер:**
    ```bash
    nc -lk 9999
    ```

2.  **Подготовьте тестовые данные** в формате: `timestamp,user_id,action,value`
    Пример:
    ```
    2024-01-15 10:00:01,user1,click,5
    2024-01-15 10:00:02,user2,purchase,100
    2024-01-15 10:00:03,user1,click,3
    ```

#### Часть 2: Чтение потоковых данных

1.  **Создайте сессию Spark с поддержкой Streaming:**
    ```python
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    
    spark = SparkSession.builder \
        .appName("StructuredStreamingLab") \
        .getOrCreate()
    ```

2.  **Определите схему данных:**
    ```python
    schema = StructType([
        StructField("timestamp", TimestampType(), True),
        StructField("user_id", StringType(), True),
        StructField("action", StringType(), True),
        StructField("value", IntegerType(), True)
    ])
    ```

3.  **Создайте потоковый DataFrame:**
    ```python
    stream_df = spark.readStream \
        .format("socket") \
        .option("host", "localhost") \
        .option("port", 9999) \
        .load()
    ```

#### Часть 3: Преобразование и агрегация данных

1.  **Парсинг и преобразование данных:**
    ```python
    parsed_df = stream_df.select(
        split(col("value"), ",").getItem(0).cast("timestamp").alias("timestamp"),
        split(col("value"), ",").getItem(1).alias("user_id"),
        split(col("value"), ",").getItem(2).alias("action"),
        split(col("value"), ",").getItem(3).cast("integer").alias("value")
    )
    ```

2.  **Скользящие агрегации:**
    *   Подсчет событий по 1-минутному окну
    *   Сумма значений по действиям пользователей
    *   Количество уникальных пользователей в окне

#### Часть 4: Вывод результатов

1.  **Настройте вывод в консоль:**
    ```python
    query = aggregated_df.writeStream \
        .outputMode("complete") \
        .format("console") \
        .option("truncate", "false") \
        .start()
    ```

2.  **Запустите поток и проверьте результаты**

### Требования к оформлению и отчёту:

1.  **Код:** Полный код PySpark в Jupyter Notebook
2.  **Скриншоты:** 
    *   Результаты агрегации в консоли
    *   Графики изменений показателей во времени
3.  **Отчёт:** 
    *   Описание архитектуры решения
    *   Анализ производительности
    *   Объяснение выбора параметров окон

### Критерии оценки:

*   **Удовлетворительно (3):** Настроен потоковый ввод, данные читаются корректно
*   **Хорошо (4):** Реализованы базовые агрегации, вывод работает стабильно
*   **Отлично (5):** Реализованы сложные оконные функции, настроена визуализация

---

**Пример ожидаемого вывода:**
```
Batch: 1
+-------------------+-----+-------------+
|window             |action|count_events|
+-------------------+-----+-------------+
|2024-01-15 10:00:00|click|15          |
|2024-01-15 10:00:00|purchase|5         |
+-------------------+-----+-------------+
```