# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ2: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–º MapReduce

## –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã
–û—Å–≤–æ–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è **MapReduce** –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ. –ü–æ–ª—É—á–∏—Ç—å –Ω–∞–≤—ã–∫–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞—Ä–∞–¥–∏–≥–º—ã MapReduce –Ω–∞ —è–∑—ã–∫–µ **Python** –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤ **Hadoop Streaming**.

---

## –°—Ç–µ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π

- **–Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:** Python 3  
- **–§—Ä–µ–π–º–≤–æ—Ä–∫:** Hadoop Streaming (JAR-—É—Ç–∏–ª–∏—Ç–∞)  
- **–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ / –û–°:** Linux (Ubuntu), –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ HDFS –∏ Hadoop  
- **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏):** mrjob  

---

## –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å

**MapReduce** ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ.  
–í—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –Ω–∞ –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∑—ã:

### 1. Map (–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
- **–í—Ö–æ–¥:** –ø–∞—Ä—ã –∫–ª—é—á‚Äì–∑–Ω–∞—á–µ–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, *(offset, —Å—Ç—Ä–æ–∫–∞_–∏–∑_—Ñ–∞–π–ª–∞)*).  
- **–§—É–Ω–∫—Ü–∏—è:** –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è `map()` –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—É—é –≤—Ö–æ–¥–Ω—É—é –ø–∞—Ä—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–±–æ—Ä –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø–∞—Ä –∫–ª—é—á‚Äì–∑–Ω–∞—á–µ–Ω–∏–µ.  
- **–í—ã—Ö–æ–¥:** —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø–∞—Ä –≤–∏–¥–∞ *(–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π_–∫–ª—é—á, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ_–∑–Ω–∞—á–µ–Ω–∏–µ)*.

### 2. Reduce (–°–≤—ë—Ä—Ç–∫–∞)
- **–í—Ö–æ–¥:** –ø–∞—Ä—ã *(–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π_–∫–ª—é—á, —Å–ø–∏—Å–æ–∫_–≤—Å–µ—Ö_–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö_–∑–Ω–∞—á–µ–Ω–∏–π)*.  
- **–§—É–Ω–∫—Ü–∏—è:** –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è `reduce()` –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–ª—é—á –∏ –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è. –û–Ω–∞ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∏–ª–∏ —Å—É–º–º–∏—Ä—É–µ—Ç —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è.  
- **–í—ã—Ö–æ–¥:** –∫–æ–Ω–µ—á–Ω—ã–µ –ø–∞—Ä—ã –∫–ª—é—á‚Äì–∑–Ω–∞—á–µ–Ω–∏–µ, –∑–∞–ø–∏—Å—ã–≤–∞–µ–º—ã–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.

–ú–µ–∂–¥—É —ç—Ç–∏–º–∏ —Ñ–∞–∑–∞–º–∏ framework –Ω–µ—è–≤–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —ç—Ç–∞–ø **Shuffle & Sort**, –∫–æ—Ç–æ—Ä—ã–π –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∫–ª—é—á–∞–º.

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### –ó–∞–¥–∞—á–∞ 1: –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π WordCount

#### –ê–ª–≥–æ—Ä–∏—Ç–º
- **Mapper (`mapper_wc.py`)** ‚Äî —á–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–≤–æ–¥–∞, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –Ω–∞ —Å–ª–æ–≤–∞ –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤—ã–¥–∞—ë—Ç –ø–∞—Ä—É *(word, 1)*.  
- **Reducer (`reducer_wc.py`)** ‚Äî –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –ø–∞—Ä—ã *(word, [1, 1, 1, ...])*, —Å—É–º–º–∏—Ä—É–µ—Ç –µ–¥–∏–Ω–∏—Ü—ã –∏ –≤—ã–¥–∞—ë—Ç –∏—Ç–æ–≥–æ–≤—É—é –ø–∞—Ä—É *(word, sum)*.

---

### –ö–æ–¥ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

#### `mapper_wc.py`
```python
#!/usr/bin/env python3
import sys

def read_input(file):
    for line in file:
        yield line.split()

def main(separator='\t'):
    data = read_input(sys.stdin)
    for words in data:
        for word in words:
            print('%s%s%d' % (word, separator, 1))

if __name__ == "__main__":
    main()
```

#### `reducer_wc.py`
```python
#!/usr/bin/env python3
import sys

def main(separator='\t'):
    current_word = None
    current_count = 0
    for line in sys.stdin:
        line = line.strip()
        word, count = line.split(separator, 1)
        try:
            count = int(count)
        except ValueError:
            continue

        if current_word == word:
            current_count += count
        else:
            if current_word:
                print('%s%s%d' % (current_word, separator, current_count))
            current_count = count
            current_word = word

    if current_word == word:
        print('%s%s%d' % (current_word, separator, current_count))

if __name__ == "__main__":
    main()
```

---

### –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ

`input.txt`
```
hello world
hello data
goodbye data
```

---

### –õ–æ–∫–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞
```bash
cat input.txt | python3 mapper_wc.py | sort | python3 reducer_wc.py
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**
```
data	2
goodbye	1
hello	2
world	1
```

üì∑ *[–°–ö–†–ò–ù–®–û–¢ 1: –õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç WordCount]*
![–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç](–õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç WordCount.jpg)
![–°–ö–†–ò–ù–®–û–¢ 1: –õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç WordCount](./images/–õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç WordCount.png)

---

### –ó–∞–ø—É—Å–∫ –≤ Hadoop Streaming

**–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ HDFS:**
```bash
hdfs dfs -mkdir -p /user/student/input_wc
hdfs dfs -put input.txt /user/student/input_wc/
hdfs dfs -rm -r /user/student/output_wc
```

**–ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞:**
```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
-files mapper_wc.py,reducer_wc.py \
-input /user/student/input_wc \
-output /user/student/output_wc \
-mapper "python3 mapper_wc.py" \
-reducer "python3 reducer_wc.py"
```

üì∑ *[–°–ö–†–ò–ù–®–û–¢ 2: –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞ Hadoop Streaming]*  
üì∑ *[–°–ö–†–ò–ù–®–û–¢ 3: –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ WordCount –≤ Hadoop]*

**–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**
```bash
hdfs dfs -ls /user/student/output_wc
hdfs dfs -cat /user/student/output_wc/part-00000
```

üì∑ *[–°–ö–†–ò–ù–®–û–¢ 4: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ WordCount –≤ HDFS]*  
üì∑ *[–°–ö–†–ò–ù–®–û–¢ 5: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã WordCount –∏–∑ HDFS]*

---

## –ó–∞–¥–∞—á–∞ 2: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ –ø–æ –ø—Ä–µ–¥–º–µ—Ç–∞–º

### –ê–ª–≥–æ—Ä–∏—Ç–º
- **Mapper (`mapper_avg.py`)** ‚Äî –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—É *(student_id, grade)*.  
- **Reducer (`reducer_avg.py`)** ‚Äî –¥–ª—è –∫–∞–∂–¥–æ–≥–æ `student_id` –≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ –µ–≥–æ –æ—Ü–µ–Ω–æ–∫.

---

### –ö–æ–¥ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

#### `mapper_avg.py`
```python
#!/usr/bin/env python3
import sys

def main(separator='\t'):
    for line in sys.stdin:
        line = line.strip()
        student_id, subject, grade = line.split(',')
        if student_id == 'student_id':
            continue
        try:
            grade = float(grade)
        except ValueError:
            continue
        print('%s%s%f' % (student_id, separator, grade))

if __name__ == "__main__":
    main()
```

#### `reducer_avg.py`
```python
#!/usr/bin/env python3
import sys

def main(separator='\t'):
    current_student = None
    sum_grades = 0
    count_grades = 0

    for line in sys.stdin:
        line = line.strip()
        student_id, grade = line.split(separator, 1)

        try:
            grade = float(grade)
        except ValueError:
            continue

        if current_student == student_id:
            sum_grades += grade
            count_grades += 1
        else:
            if current_student:
                average_grade = sum_grades / count_grades
                print('%s%s%.1f' % (current_student, separator, average_grade))
            current_student = student_id
            sum_grades = grade
            count_grades = 1

    if current_student == student_id:
        average_grade = sum_grades / count_grades
        print('%s%s%.1f' % (current_student, separator, average_grade))

if __name__ == "__main__":
    main()
```

---

### –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ

`grades.txt`
```
student_id,subject,grade
101,math,92
101,physics,85
102,math,90
102,physics,95
103,math,80
```

---

### –õ–æ–∫–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞
```bash
cat grades.txt | python3 mapper_avg.py | sort | python3 reducer_avg.py
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**
```
101	88.5
102	92.5
103	80.0
```

üì∑ *[–°–ö–†–ò–ù–®–û–¢ 6: –õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏]*

---

### –ó–∞–ø—É—Å–∫ –≤ Hadoop Streaming

**–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ HDFS:**
```bash
hdfs dfs -mkdir -p /user/student/input_avg
hdfs dfs -put grades.txt /user/student/input_avg/
hdfs dfs -rm -r /user/student/output_avg
```

**–ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞:**
```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
-files mapper_avg.py,reducer_avg.py \
-input /user/student/input_avg \
-output /user/student/output_avg \
-mapper "python3 mapper_avg.py" \
-reducer "python3 reducer_avg.py"
```

üì∑ *[–°–ö–†–ò–ù–®–û–¢ 7: –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ –≤ Hadoop]*

**–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**
```bash
hdfs dfs -cat /user/student/output_avg/part-00000
```

üì∑ *[–°–ö–†–ò–ù–®–û–¢ 8: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ –∏–∑ HDFS]*

---

## –û—Ç–≤–µ—Ç—ã –Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã

### 1. –ü–æ—á–µ–º—É —ç—Ç–∞–ø Shuffle & Sort —è–≤–ª—è–µ—Ç—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º –∏ —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ?

**–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å:**
- –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤—Å–µ –ø–∞—Ä—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –∫–ª—é—á–æ–º –ø–æ–ø–∞–¥—É—Ç –∫ –æ–¥–Ω–æ–º—É –∏ —Ç–æ–º—É –∂–µ Reducer‚Äô—É  
- –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª—é—á–∞–º  
- –ü–æ–∑–≤–æ–ª—è–µ—Ç Reducer‚Äô–∞–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ  

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
- **Shuffle (–ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ):** –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –ø–∞—Ä—ã –∫–ª—é—á‚Äì–∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç –≤—Å–µ—Ö Mapper‚Äô–æ–≤ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –Ω–∞ —É–∑–ª—ã —Å Reducer‚Äô–∞–º–∏.  
- **Sort (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞):** –ø–∞—Ä—ã —Å–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –ø–æ –∫–ª—é—á—É, —á—Ç–æ–±—ã Reducer –º–æ–≥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞.

–ë–µ–∑ —ç—Ç–æ–≥–æ —ç—Ç–∞–ø–∞ Reducer –Ω–µ —Å–º–æ–≥ –±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, —Ç–∞–∫ –∫–∞–∫ –ø–æ–ª—É—á–∞–ª –±—ã —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–∞ –æ—Ç —Ä–∞–∑–Ω—ã—Ö Mapper‚Äô–æ–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ.

---

### 2. –í —á—ë–º –∑–∞–∫–ª—é—á–∞—é—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ MapReduce, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ –ø–æ—è–≤–ª–µ–Ω–∏—é Spark?

**–û—Å–Ω–æ–≤–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
- –ú–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å –Ω–∞ –¥–∏—Å–∫ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Ñ–∞–∑—ã (Map, Shuffle, Reduce)  
- –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ (–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≥—Ä–∞—Ñ—ã)  
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–ø–æ—á–µ–∫ –∑–∞–¥–∞—á  
- –î–æ–ª–≥–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á (–Ω–µ–ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)  
- –ñ—ë—Å—Ç–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ Map‚ÄìShuffle‚ÄìReduce

**Spark —Ä–µ—à–∞–µ—Ç —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã –∑–∞ —Å—á—ë—Ç:**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è **RDD (Resilient Distributed Datasets)**  
- –û–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏  
- DAG-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è  
- –ü–æ–¥–¥–µ—Ä–∂–∫–∏ SQL, ML –∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–í —Ö–æ–¥–µ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ **MapReduce**.  
–û–±–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –±—ã–ª–∏ –∑–∞–ø—É—â–µ–Ω—ã –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω–æ, —Ç–∞–∫ –∏ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π —Å—Ä–µ–¥–µ **Hadoop**, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ —Ä–∞–±–æ—Ç—ã MapReduce.
